---
title: "Análisis del mercado inmobiliario en Estados Unidos"
author: "Emilio Coronado de Palma"
abstract: "Este informe segmenta los mercados inmobiliarios estadounidenses mediante PCA y K-Means, considerando características de mercado, puntos de interés y datos demográficos. Se documenta la metodología, se interpretan los componentes principales, se perfilan los clústeres y se muestra su distribución espacial."
format:
  html:
    code-link: true
    toc: true
    toc-depth: 3
    theme: cosmo
editor: visual
---

## Descripción general del proyecto

Este informe presenta una segmentación del mercado inmobiliario estadounidense basada en datos a nivel de **código postal/ciudad**. Combinamos indicadores de vivienda (p. ej., precio medio de venta/listado, inventario, días en el mercado) con puntos de interés (POI) de OSM y datos demográficos. Nuestro flujo de trabajo se centra en la estandarización, la reducción de dimensionalidad (**PCA**), la agrupación (**K-Means**) y la visualización geoespacial. Mantenemos el código sin cambios y priorizamos una interpretación clara y reproducible de cada paso.

Preguntas clave que abordamos: - *¿Qué dimensiones latentes resumen mejor la variación entre ubicaciones?* - *¿Cuántos segmentos de mercado (clústeres) distintos emergen?* - *¿Qué caracteriza a cada clúster en términos de señales de mercado, POI y datos demográficos?* - *¿Dónde se ubican geográficamente estos clústeres?*

> Nota: Este documento no incluye predicción supervisada de precios. El énfasis está en el descubrimiento e interpretación de la estructura. Pero sería una buena actualización para el proyecto en el futuro.

## Carga de datos

Empezamos cargando el dataset principal desde Kaggle (HousingTS) y hacemos una pequeña exploración para comprenderlo mejor. *Este es el link del dataset:* https://www.kaggle.com/datasets/shengkunwang/housets-dataset

```{python}
#| echo: false
#| include: false
import pandas as pd

# 1) Cargar datos
df = pd.read_csv("C:/EMILIO/PROYECTOS PERSONALES EN PYTHON/US_housing_clustering/data/HouseTS.csv")

df.head()
df.info()
df.describe(include='all')
```

Como este proyecto lo estamos realizado a través de positron visualizamos en el propio IDE el dataset y vemos que el porcentaje de datos missing es 0 para todas las variables. Es un dataset limpio y listo para analizar.

# Análisis de componentes principales (PCA)

Aplicamos **PCA** para capturar la variación dominante entre ubicaciones, reteniendo una proporción predefinida de varianza explicada del **90%**.

```{python}
#| eval: true
#| echo: false
#| include: true

# PCA básico por ZIP para clustering
#carga de librerías
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
import matplotlib as mpl
import matplotlib.pyplot as plt



# 2) Ajustamos el formato de la fecha
df["date"] = pd.to_datetime(df["date"])

# 3) Seleccionamos columnas numéricas y excluimos identificadores/fecha/date
numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()
cols_excluir = ["zipcode", "year", "date"]  # dejamos fuera IDs, año y fecha
feature_cols = [c for c in numeric_cols if c not in cols_excluir]

# 4) Agregamos la media en el tiempo por codigo postal, por que no queremos tener todas las series temporales de cada código postal.
agg_num = df.groupby("zipcode")[feature_cols].mean()

# 5) Recuperar una ciudad representativa por ZIP (aunque debería estar bien segmentados ya, solo por asegurar) (la moda; si empata, el primero)
def ciudad_moda(s):
    m = s.mode()
    return m.iloc[0] if not m.empty else s.iloc[0]

zip_city = df.groupby("zipcode")["city"].agg(ciudad_moda)
agg = agg_num.join(zip_city)

# 6) Estandarizamos variables
X = agg_num.values
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 7) PCA reteniendo el 90% de la varianza
pca = PCA(n_components=0.90, random_state=0)
X_pca = pca.fit_transform(X_scaled)

# 8) Unimos los resultado en el DataFrame para clusterizar posteriormente
pca_cols = [f"PC{i+1}" for i in range(X_pca.shape[1])]
pca_df = pd.DataFrame(X_pca, index=agg.index, columns=pca_cols).reset_index()
pca_df = pca_df.merge(zip_city.reset_index(), on="zipcode", how="left")

# 9) Info rápida de varianza explicada
explained = pca.explained_variance_ratio_
cum_explained = explained.cumsum()

print("Número de componentes principales:", len(pca_cols))
print("Varianza explicada por componente:", explained)
print("Varianza explicada acumulada:", cum_explained)
```

Ahora vamos a visualizar la interpretación de PC1 y PC2. Esto nos ayuda a resumir los gradientes generales de mercado, demográficos y de puntos de interés.

```{python}
#| eval: true
#| echo: true
#| include: true
plt.figure(figsize=(8,6))
plt.scatter(pca_df["PC1"], pca_df["PC2"], s=8, alpha=0.5)
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title("Espacio PCA = PC1 vs PC2")
plt.show()
```

Ahora vamos a realizar la clusterización y vamos a visualizar el mismo gráfico de dispersion de las componentes principales ya segmentado.

# Clusterización con k-means

```{python}
#| eval: true
#| echo: false
#| include: true

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Nos quedamos solo con las componentes principales:
pca_cols = [c for c in pca_df.columns if c.startswith("PC")]
X = pca_df[pca_cols].values

# --- 2) Elegir k de forma rápida con la silueta ---
k_range = range(2, 11)
sil_scores = []

for k in k_range:
    km = KMeans(n_clusters=k, n_init="auto", random_state=42)
    labels = km.fit_predict(X)
    sil = silhouette_score(X, labels)
    sil_scores.append(sil)

# k “óptimo” (máxima silueta)
best_k = k_range[int(np.argmax(sil_scores))]
print("k con mejor silueta:", best_k)

# --- 3) Ajustar K-means definitivo con ese k ---
kmeans = KMeans(n_clusters=best_k, n_init="auto", random_state=42)
labels = kmeans.fit_predict(X)

# Añadir etiquetas al Dataframe
pca_df["cluster"] = labels

# --- 4) Visualizar en PC1 vs PC2 coloreado por cluster ---
plt.figure(figsize=(9,6))
scatter = plt.scatter(pca_df["PC1"], pca_df["PC2"], c=pca_df["cluster"], s=10, alpha=0.6)
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.title(f"K-means en espacio PCA (k={best_k})")
plt.show()


```

Ahora ya visualizamos muy bien cómo se distribuyen estos clusters en función de la PC1 y la PC2.

Ahora vamos a graficar una tabla para entender estos clusters mejor. Pero primero tenemos que entender qué variables son las que más información nos dan. Por tanto medimos la varianza entre los clusters y obtenemos la lista de variables que mayor varianza dan.

```{python}
#| eval: true
#| echo: false
#| include: true

# 1. Seleccionar variables numéricas a usar
num_cols = [
    c for c in df.columns
    if df[c].dtype != 'object' and c not in ['year', 'date']
]

# 2. Agregar por ZIP (media temporal)
agg = df.groupby("zipcode")[num_cols].mean()

# 3. Añadir cluster
agg = agg.merge(
    pca_df[["zipcode", "cluster"]].set_index("zipcode"),
    left_index=True,
    right_index=True,
    how="left"
)



# 4. Estandarizar todas las variables numéricas
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
agg_scaled = pd.DataFrame(
    scaler.fit_transform(agg[num_cols]),
    columns=num_cols,
    index=agg.index
)

# 5. Calcular medias por cluster con variables estandarizadas
cluster_stats_std = agg_scaled.join(agg["cluster"]).groupby("cluster").mean()

# 6. Varianza entre clusters (correcta)
cluster_variability = cluster_stats_std.var(axis=0).sort_values(ascending=False)
cluster_variability.head(20)


```

Una vez teniendo las variables que más información aportan a la diferenciación y al entendimiento de estos clusters. Generamos la tabla.

```{python}
#| eval: true
#| echo: false
#| include: false

# =========================
# PERFIL DE CLUSTERS (TABLA)
# =========================
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler

# --- ENTRADAS requeridas ---
# agg: DataFrame con tus observaciones (índice = id), columnas numéricas y la columna 'cluster'
# num_cols: lista de columnas numéricas a estandarizar
# Ej. ya lo tienes según tu mensaje:
# agg, num_cols, agg["cluster"]

# 1) Mapa de importancias (lo que compartiste)
importance = {
    "station": 6.292687,
    "restaurant": 6.033795,
    "supermarket": 5.931101,
    "bank": 5.798149,
    "park": 5.090172,
    "hospital": 4.288910,
    "school": 3.899366,
    "median_list_ppsf": 2.905003,
    "median_ppsf": 2.526744,
    "bus": 2.178003,
    "Median Home Value": 1.285565,
    "price": 1.164426,
    "median_sale_price": 1.155048,
    "Total Housing Units": 0.807132,
    "mall": 0.791088,
    "median_list_price": 0.729817,
    "Total Labor Force": 0.695403,
    "Median Commute Time": 0.679908,
    "Per Capita Income": 0.656566,
    "Total School Enrollment": 0.654586,
}

# Aseguramos que solo usamos variables disponibles
available_vars = [c for c in num_cols if c in importance]
if len(available_vars) == 0:
    raise ValueError("Ninguna de las variables de 'importance' está en num_cols.")

# 2) Estandarización (z-score) — tal como ya venías haciendo
scaler = StandardScaler()
agg_scaled = pd.DataFrame(
    scaler.fit_transform(agg[num_cols]),
    columns=num_cols,
    index=agg.index
)

# 3) Medias por cluster en escala estandarizada
cluster_stats_std = (
    agg_scaled.join(agg["cluster"])
    .groupby("cluster")
    .mean()
)

# 4) Orden de columnas según tu importancia (desc)
ordered_vars = sorted(available_vars, key=lambda v: importance[v], reverse=True)

# 5) Tabla principal: perfiles de cluster (z-scores)
cluster_profile = cluster_stats_std[ordered_vars].copy()


# 8) Estilizado de la tabla principal para visualización clara
#    - Gradiente rojo/azul centrado en 0

Z_THRESHOLD = 1.0

def highlight_strong(v, thr=Z_THRESHOLD):
    if pd.isna(v):
        return ""
    return "font-weight: bold;" if abs(v) >= thr else ""

# Ajusta el rango para que se vea bien centrado (z-scores)
zmin, zmax = -2, 2  # puedes ampliar a (-3, 3) si hay valores extremos

styler = (
    cluster_profile.style
    .background_gradient(cmap="coolwarm", axis=None, vmin=zmin, vmax=zmax)
    .map(highlight_strong)
    .format("{:.2f}")
    .set_caption("Perfiles de cluster (z-scores estandarizados) — columnas ordenadas por importancia")
)

styler

```

Ahora ya podemos interpretar los resultados y definir los 4 clusters.

**Cluster 0 — “Zonas rezagadas o de baja actividad urbana”**

Prácticamente todas las variables están en negativo, la mayoría entre –0.2 y –0.7.

Destacan valores bajos en:

station, restaurant, supermarket, bank, park, hospital, school → indica baja densidad de servicios y equipamientos.

Total Housing Units, Total Labor Force, Per Capita Income, Total School Enrollment también por debajo de la media → zonas menos pobladas y con menor dinamismo económico.

En conjunto: este grupo representa áreas periféricas o rurales, con menor acceso a infraestructura, menor actividad económica y precios inmobiliarios más bajos.

**Cluster 1 — “Zonas residenciales estables”**

La mayoría de los z-scores rondan 0, lo que indica valores promedio respecto al conjunto.

Algunas variables ligeramente por encima del promedio:

mall, park, school, Total Housing Units, Total Labor Force, Median Commute Time, Total School Enrollment → áreas con actividad moderada, algo más residenciales y con cierta dotación de servicios.

Variables por debajo de la media en median_list_price, price, median_ppsf → precios algo más bajos que el promedio.

En conjunto: zonas residenciales medias, equilibradas, con buena dotación pero sin excesos de precios o servicios extremos.

**Cluster 2 — “Núcleos urbanos altamente desarrollados”**

Es el grupo más extremo y distintivo:

z-scores de +4 a +5 en casi todas las variables principales (station, restaurant, supermarket, bank, park, hospital, school).

También valores altos en variables económicas e inmobiliarias (median_list_ppsf, median_ppsf, price, Median Home Value).

Representa claramente las áreas centrales o premium de la ciudad:

Altísima concentración de servicios, infraestructura y precios de vivienda elevados.

Alta densidad poblacional y actividad económica.

Este es el cluster de mayor centralidad, riqueza y desarrollo urbano.

**Cluster 3 — “Zonas en transición o subcentros emergentes”**

z-scores positivos moderados (entre +0.3 y +1.5) en muchas variables.

Destacan:

median_list_ppsf, median_ppsf, median_sale_price, Median Home Value, Per Capita Income → por encima de la media.

station, restaurant, supermarket, park, hospital → también algo superiores.

A diferencia del cluster 2, los valores no son extremos → zonas urbanas consolidadas pero no top, posiblemente barrios intermedios o en expansión, con precios en crecimiento y buena accesibilidad.

## Visualizaciones Geoespaciales con geopandas.

La cartografía la obtenemos del census estadounidense aquí: https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2020&layergroup=ZIP%20Code%20Tabulation%20Areas

#### Lectura de los datos geoespaciales.

```{python}
#| eval: true
#| echo: true
#| include: true
import geopandas as gpd

zcta = gpd.read_file("C:/EMILIO/PROYECTOS PERSONALES EN PYTHON/US_housing_clustering/data/Shapefiles/tl_2020_us_zcta520.shp")

zcta.head()

#ajustamos el dataframe para tener solo el zipcode y el cluster asociado.
df_map = pca_df[["zipcode", "cluster","city"]].drop_duplicates()

#Como en el archivo .shp tiene el zipcode con 5 dígitos hay que actualizar la variable en este df para matchear
df_map["zipcode_str"] = df_map["zipcode"].astype(str).str.zfill(5)

#renombramos la variable para que tengan el mismo nombre en ambos datframes.
zcta = zcta.rename(columns={"ZCTA5CE20": "zipcode_str"})

#mergeamos para tenerlo en una sola BBDD
gdf = zcta.merge(df_map, on="zipcode_str", how="left")


gdf.plot(column="cluster", categorical=True, legend=True, figsize=(12,8))

```

```{python}
# Filtrar por una ciudad
ciudad = "NY"  # cámbialo por el nombre de la ciudad que te interese
gdf_city = gdf[gdf["city"] == ciudad].copy()

# Revisar cuántos clusters hay dentro de esa ciudad
print(gdf_city["cluster"].value_counts())

import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(10, 8))
gdf_city.plot(column="cluster", categorical=True, legend=True, cmap="Set2", ax=ax)
ax.set_title(f"Clusters en {ciudad}", fontsize=14)
ax.axis("off")
plt.show()

```

```{python}
#| eval: true
#| echo: false
#| include: true
import contextily as ctx

# Proyectar a Web Mercator (necesario para tiles de contexto)
gdf_city_web = gdf_city.to_crs(epsg=3857)

fig, ax = plt.subplots(figsize=(10, 8))
gdf_city_web.plot(column="cluster", categorical=True, legend=True, cmap="Set2", ax=ax, alpha=0.8)
ctx.add_basemap(ax, source="CartoDB.Positron")
ax.set_title(f"Clusters en {ciudad}", fontsize=14)
ax.axis("off")
plt.show()

```

Le incorporamos zoom para poder verlo desde más cerca.

```{python}
#| eval: true
#| echo: false
#| include: true
# ============================
# MAPA DE CLUSTERS POR CIUDAD
# ============================
import geopandas as gpd
import matplotlib.pyplot as plt
import contextily as ctx

def plot_city_clusters(
    gdf,
    ciudad,
    city_col="city",          # cambia si tu columna se llama distinto (ej. "municipio")
    cluster_col="cluster",
    zoom_method="factor",     # "factor" o "radius"
    zoom_factor=0.35,         # si zoom_method="factor": 0.9 poco zoom, 0.35 zoom fuerte
    radius_km=25,             # si zoom_method="radius": radio alrededor del centro
    figsize=(10, 8),
    cmap="Set2",
    basemap="CartoDB.Positron",
    edgecolor="black",
    linewidth=0.3,
    alpha=0.85,
    title_prefix="Clusters en "
):
    # 1) Filtrar ciudad
    gdf_city = gdf[gdf[city_col] == ciudad].copy()
    if gdf_city.empty:
        raise ValueError(f"No hay geometrías para '{ciudad}' en {city_col}.")

    # 2) Asegurar CRS Web Mercator para el basemap
    gdf_city_web = gdf_city.to_crs(epsg=3857)

    # 3) Plot base
    fig, ax = plt.subplots(figsize=figsize)
    gdf_city_web.plot(
        column=cluster_col,
        categorical=True,
        legend=True,
        cmap=cmap,
        ax=ax,
        alpha=alpha,
        edgecolor=edgecolor,
        linewidth=linewidth
    )

    # 4) Basemap
    ctx.add_basemap(ax, source=basemap)

    # 5) ZOOM
    if zoom_method == "factor":
        # Zoom relativo a la extensión de la ciudad
        minx, miny, maxx, maxy = gdf_city_web.total_bounds
        x_margin = (maxx - minx) * (1 - zoom_factor) / 2
        y_margin = (maxy - miny) * (1 - zoom_factor) / 2
        ax.set_xlim(minx + x_margin, maxx - x_margin)
        ax.set_ylim(miny + y_margin, maxy - y_margin)
    elif zoom_method == "radius":
        # Zoom por centro geométrico + radio en km
        center = gdf_city_web.unary_union.centroid
        x, y = center.x, center.y
        buffer = radius_km * 1000
        ax.set_xlim(x - buffer, x + buffer)
        ax.set_ylim(y - buffer, y + buffer)
    else:
        raise ValueError("zoom_method debe ser 'factor' o 'radius'.")

    # 6) Estética final
    ax.set_title(f"{title_prefix}{ciudad}", fontsize=16)
    ax.axis("off")
    plt.tight_layout()
    plt.show()



plot_city_clusters(
    gdf,
    ciudad="NY",
    city_col="city",          # cambia si tu columna se llama distinto
    cluster_col="cluster",
    zoom_method="factor",
    zoom_factor=0.35          # baja a 0.25 si quieres aún más zoom
)

```

```{python}
#| eval: true
#| echo: false
#| include: true

import geopandas as gpd
import matplotlib.pyplot as plt
import contextily as ctx
from shapely.geometry import Point
import pyproj

# === Parámetros de entrada ===
ciudad = "NY"  # tu filtro de ciudad
city_col = "city"
cluster_col = "cluster"

# === Filtrar la ciudad y convertir a Web Mercator ===
gdf_city = gdf[gdf[city_col] == ciudad].copy()
gdf_city_web = gdf_city.to_crs(epsg=3857)

# === Crear el plot ===
fig, ax = plt.subplots(figsize=(10, 8))
gdf_city_web.plot(
    column=cluster_col,
    categorical=True,
    legend=True,
    cmap="Set2",
    ax=ax,
    alpha=0.85,
    edgecolor="black",
    linewidth=0.3
)

# === Añadir basemap ===
ctx.add_basemap(ax, source="CartoDB.Positron")

# === ZOOM CENTRADO MANUALMENTE (NYC core) ===
# Coordenadas aproximadas de Manhattan (EPSG:4326 → luego se convierte)
lon, lat = -73.97, 40.75  # centro aproximado de Manhattan
radius_km = 25            # tamaño de zoom (20–30 km funciona bien)

# Convertir a EPSG:3857
proj = pyproj.Transformer.from_crs("EPSG:4326", "EPSG:3857", always_xy=True)
x, y = proj.transform(lon, lat)

buffer = radius_km * 1000
ax.set_xlim(x - buffer, x + buffer)
ax.set_ylim(y - buffer, y + buffer)

# === Estética final ===
ax.set_title(f"Clusters en {ciudad}", fontsize=16)
ax.axis("off")
plt.tight_layout()
plt.show()

```

```{python}
#| eval: true
#| echo: false
#| include: true

# ==============================
# MAPA de NY por CLUSTERS con zoom interactivo
# ==============================

import geopandas as gpd

ciudad = "NY"          # cambia si quieres otra
city_col = "city"      # cambia si tu columna se llama distinto
cluster_col = "cluster"

# Filtra y pasa a lat/lon (WGS84) para web
gdf_city = gdf[gdf[city_col] == ciudad].to_crs(epsg=4326)

m = gdf_city.explore(
    column=cluster_col,          # colorea por cluster
    categorical=True,
    cmap="Set2",
    tiles="CartoDB positron",    # mapa base
    legend=True                  # leyenda de clusters
    # sin tooltip ni popup
)

m.save("mapa_clusters_interactivo.html")
print("Mapa interactivo guardado en 'mapa_clusters_interactivo.html'")

```

```{python}
#| eval: false
#| include: true
gdf_w = gdf.to_crs(epsg=4326)

mG = gdf_w.explore(
    column="cluster",
    categorical=True,
    cmap="Set2",
    tiles="CartoDB positron",
    legend=True
)

mG.save("mapa_clusters_interactivo_global.html")
print("Mapa interactivo guardado en 'mapa_clusters_interactivo_global.html'")

```